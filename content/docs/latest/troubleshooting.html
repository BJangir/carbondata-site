<!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.
--><h1>Troubleshooting</h1>
<p>This tutorial is designed to provide troubleshooting for end users and developers
    who are building, deploying, and using CarbonData.</p>
<h2 id="failed-to-load-thrift-libraries">Failed to load thrift libraries</h2>
<p><strong>Symptom</strong></p>
<p>Thrift throws following exception :</p>
<pre><code>thrift: error while loading shared libraries:
libthriftc.so.0: cannot open shared object file: No such file or directory
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The complete path to the directory containing the libraries is not configured correctly.</p>
<p><strong>Procedure</strong></p>
<p>Follow the steps below to ensure loading of libraries appropriately :</p>
<ol>
    <li>
        <p>For ubuntu you have to add a custom.conf file to /etc/ld.so.conf.d<br>
            For example,</p>
        <pre><code>sudo gedit /etc/ld.so.conf.d/randomLibs.conf
</code></pre>
        <p>Inside this file you are supposed to configure the complete path to the directory that
            contains all the libraries that you wish to add to the system, let us say
            /home/ubuntu/localLibs</p>
    </li>
    <li>
        <p>To ensure your library location ,check for existence of <a href="http://libthrift.so">libthrift.so</a>
        </p>
    </li>
    <li>
        <p>Save and run the following command to update the system with this libs.</p>
        <pre><code>sudo ldconfig
</code></pre>
    </li>
</ol>
Note : Remember to add only the path to the directory, not the full path for that file, all the libraries inside that path will be automatically indexed.
<h2 id="failed-to-launch-the-spark-shell">Failed to launch the Spark Shell</h2>
<p><strong>Symptom</strong></p>
<p>The shell prompts the following error :</p>
<pre><code>org.apache.spark.sql.CarbonContext$$anon$$apache$spark$sql$catalyst$analysis
$OverrideCatalog$_setter_$org$apache$spark$sql$catalyst$analysis
$OverrideCatalog$$overrides_$e
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The Spark Version and the selected Spark Profile do not match.</p>
<p><strong>Procedure</strong></p>
<ol>
    <li>
        <p>Ensure your spark version and selected profile for spark are correct.</p>
    </li>
    <li>
        <p>Use the following command :</p>
    </li>
</ol>
<pre><code>
 &quot;mvn -Pspark-2.1 -Dspark.version {yourSparkVersion} clean package&quot;

</code></pre>
Note :  Refrain from using &quot;mvn clean package&quot; without specifying the profile.
<h2 id="failed-to-execute-load-query-on-cluster">Failed to execute load query on cluster.
</h2>
<p><strong>Symptom</strong></p>
<p>Load query failed with the following exception:</p>
<pre><code>Dictionary file is locked for updation.
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The carbon.properties file is not identical in all the nodes of the cluster.</p>
<p><strong>Procedure</strong></p>
<p>Follow the steps to ensure the carbon.properties file is consistent across all the nodes:</p>
<ol>
    <li>
        <p>Copy the carbon.properties file from the master node to all the other nodes in the
            cluster.<br>
            For example, you can use ssh to copy this file to all the nodes.</p>
    </li>
    <li>
        <p>For the changes to take effect, restart the Spark cluster.</p>
    </li>
</ol>
<h2 id="failed-to-execute-insert-query-on-cluster">Failed to execute insert query on
    cluster.</h2>
<p><strong>Symptom</strong></p>
<p>Load query failed with the following exception:</p>
<pre><code>Dictionary file is locked for updation.
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The carbon.properties file is not identical in all the nodes of the cluster.</p>
<p><strong>Procedure</strong></p>
<p>Follow the steps to ensure the carbon.properties file is consistent across all the nodes:</p>
<ol>
    <li>
        <p>Copy the carbon.properties file from the master node to all the other nodes in the
            cluster.<br>
            For example, you can use scp to copy this file to all the nodes.</p>
    </li>
    <li>
        <p>For the changes to take effect, restart the Spark cluster.</p>
    </li>
</ol>
<h2 id="failed-to-connect-to-hiveuser-with-thrift">Failed to connect to hiveuser with
    thrift</h2>
<p><strong>Symptom</strong></p>
<p>We get the following exception :</p>
<pre><code>Cannot connect to hiveuser.
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The external process does not have permission to access.</p>
<p><strong>Procedure</strong></p>
<p>Ensure that the Hiveuser in mysql must allow its access to the external processes.</p>
<h2 id="failure-to-read-the-metastore-db-during-table-creation">Failure to read the
    metastore db during table creation.</h2>
<p><strong>Symptom</strong></p>
<p>We get the following exception on trying to connect :</p>
<pre><code>Cannot read the metastore db
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The metastore db is dysfunctional.</p>
<p><strong>Procedure</strong></p>
<p>Remove the metastore db from the carbon.metastore in the Spark Directory.</p>
<h2 id="failed-to-load-data-on-the-cluster">Failed to load data on the cluster</h2>
<p><strong>Symptom</strong></p>
<p>Data loading fails with the following exception :</p>
<pre><code>Data Load failure exeception
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The following issue can cause the failure :</p>
<ol>
    <li>
        <p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent
            across all nodes of the cluster.</p>
    </li>
    <li>
        <p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p>
    </li>
</ol>
<p><strong>Procedure</strong></p>
<p>Follow the steps to ensure the following configuration files are consistent across all the
    nodes:</p>
<ol>
    <li>
        <p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master
            node to all the other nodes in the cluster.<br>
            For example, you can use scp to copy this file to all the nodes.</p>
        <p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p>
    </li>
    <li>
        <p>For the changes to take effect, restart the Spark cluster.</p>
    </li>
</ol>
<h2 id="failed-to-insert-data-on-the-cluster">Failed to insert data on the cluster</h2>
<p><strong>Symptom</strong></p>
<p>Insertion fails with the following exception :</p>
<pre><code>Data Load failure exeception
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>The following issue can cause the failure :</p>
<ol>
    <li>
        <p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent
            across all nodes of the cluster.</p>
    </li>
    <li>
        <p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p>
    </li>
</ol>
<p><strong>Procedure</strong></p>
<p>Follow the steps to ensure the following configuration files are consistent across all the
    nodes:</p>
<ol>
    <li>
        <p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master
            node to all the other nodes in the cluster.<br>
            For example, you can use scp to copy this file to all the nodes.</p>
        <p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p>
    </li>
    <li>
        <p>For the changes to take effect, restart the Spark cluster.</p>
    </li>
</ol>
<h2 id="failed-to-execute-concurrent-operations">Failed to execute Concurrent Operations.
</h2>
<p><strong>Symptom</strong></p>
<p>Execution of Concurrent Operations (Load,Insert,Update) on table by multiple workers fails with
    the following exception :</p>
<pre><code>Table is locked for updation.
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>Concurrency not supported.</p>
<p><strong>Procedure</strong></p>
<p>Worker must wait for the query execution to complete and the table to release the lock for
    another query execution to succeedâ€¦</p>
<h2 id="failed-to-create-a-table-with-a-single-numeric-column">Failed to create a table
    with a single numeric column.</h2>
<p><strong>Symptom</strong></p>
<p>Execution fails with the following exception :</p>
<pre><code>Table creation fails.
</code></pre>
<p><strong>Possible Cause</strong></p>
<p>Behavior not supported.</p>
<p><strong>Procedure</strong></p>
<p>A single column that can be considered as dimension is mandatory for table creation.</p>

