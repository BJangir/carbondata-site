<!--
    Licensed to the Apache Software Foundation (ASF) under one
    or more contributor license agreements.  See the NOTICE file
    distributed with this work for additional information
    regarding copyright ownership.  The ASF licenses this file
    to you under the Apache License, Version 2.0 (the
    "License"); you may not use this file except in compliance
    with the License.  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

    Unless required by applicable law or agreed to in writing,
    software distributed under the License is distributed on an
    "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
    KIND, either express or implied.  See the License for the
    specific language governing permissions and limitations
    under the License.
--><h1>Troubleshooting</h1><p>This tutorial is designed to provide troubleshooting for end users and developers who are building, deploying, and using CarbonData.</p>
<ul>
  <li><a href="#failed-to-load-thrift-libraries">Failed to load thrift libraries</a></li>
  <li><a href="#failed-to-launch-the-spark-shell">Failed to launch the Spark Shell</a></li>
  <li><a href="#query-failure-with-generic-error-on-the-beeline">Query Failure with Generic Error on the Beeline</a></li>
  <li><a href="#failed-to-execute-load-query-on-cluster">Failed to execute load query on cluster</a></li>
  <li><a href="#failed-to-execute-insert-query-on-cluster">Failed to execute insert query on cluster</a></li>
  <li><a href="#failed-to-connect-to-hiveuser-with-thrift">Failed to connect to hiveuser with thrift</a></li>
  <li><a href="#failure-to-read-the-metastore-db-during-table-creation">Failure to read the metastore db during table creation</a></li>
  <li><a href="#failed-to-load-data-on-the-cluster">Failed to load data on the cluster</a></li>
  <li><a href="#failed-to-insert-data-on-the-cluster">Failed to insert data on the cluster</a></li>
  <li><a href="#failed-to-execute-concurrent-operations">Failed to execute Concurrent Operations</a></li>
  <li><a href="#failed-to-create-a-table-with-a-single-numeric-column">Failed to create a table with a single numeric column</a></li>
  <li><a href="#data-failure-because-of-bad-records">Data Failure because of Bad Records</a></li>
</ul><h2>Failed to load thrift libraries</h2><p><strong>Symptom</strong></p><p>Thrift throws following exception :</p><p><code>
  thrift: error while loading shared libraries:
  libthriftc.so.0: cannot open shared object file: No such file or directory
</code></p><p><strong>Possible Cause</strong></p><p>The complete path to the directory containing the libraries is not configured correctly.</p><p><strong>Procedure</strong></p><p>Follow the steps below to ensure loading of libraries appropriately :</p>
<ol>
  <li><p>For ubuntu you have to add a custom.conf file to /etc/ld.so.conf.d  For example,</p><p><code>
 sudo gedit /etc/ld.so.conf.d/randomLibs.conf
</code></p><p>Inside this file you are supposed to configure the complete path to the directory that contains all the libraries that you wish to add to the system, let us say /home/ubuntu/localLibs</p></li>
  <li><p>To ensure your library location ,check for existence of libthrift.so</p></li>
  <li><p>Save and run the following command to update the system with this libs.</p><p><code>
  sudo ldconfig
</code></p><p>Note : Remember to add only the path to the directory, not the full path for that file, all the libraries inside that path will be automatically indexed.</p></li>
</ol><h2>Failed to launch the Spark Shell</h2><p><strong>Symptom</strong></p><p>The shell prompts the following error :</p><p><code>
  org.apache.spark.sql.CarbonContext$$anon$$apache$spark$sql$catalyst$analysis
  $OverrideCatalog$_setter_$org$apache$spark$sql$catalyst$analysis
  $OverrideCatalog$$overrides_$e
</code></p><p><strong>Possible Cause</strong></p><p>The Spark Version and the selected Spark Profile do not match.</p><p><strong>Procedure</strong></p>
<ol>
  <li><p>Ensure your spark version and selected profile for spark are correct.</p></li>
  <li><p>Use the following command :</p><p><code>
 &quot;mvn -Pspark-2.1 -Dspark.version {yourSparkVersion} clean package&quot;
</code></p><p>Note : Refrain from using "mvn clean package" without specifying the profile.</p></li>
</ol><h2>Query Failure with Generic Error on the Beeline</h2><p><strong>Symptom</strong></p><p>Query fails on the executor side and generic error message is printed on the beeline console</p><p><img src="../../../src/site/markdown/images/query_failure_beeline.png?raw=true" alt="Query Failure Beeline" /></p><p><strong>Possible Causes</strong></p>
<ul>
  <li>In Query flow, Table B-Tree will be loaded into memory on the driver side and filter condition is validated against the min-max of each block to identify false positive,  Once the blocks are selected, based on number of available executors, blocks will be distributed to each executor node as shown in below driver logs snapshot</li>
</ul><p><img src="../../../src/site/markdown/images/query_failure_logs.png?raw=true" alt="Query Failure Logs" /></p>
<ul>
  <li><p>When the error occurs in driver side while b-tree loading or block distribution, detail error message will be printed on the beeline console and error trace will be printed on the driver logs.</p></li>
  <li><p>When the error occurs in the executor side, generic error message will be printed as shown in issue description.</p></li>
</ul><p><img src="../../../src/site/markdown/images/query_failure_job_details.png?raw=true" alt="Query Failure Job Details" /></p>
<ul>
  <li>Details of the failed stages can be seen in the Spark Application UI by clicking on the failed stages on the failed job as shown in previous snapshot</li>
</ul><p><img src="../../../src/site/markdown/images/query_failure_spark_ui.png?raw=true" alt="Query Failure Spark UI" /></p><p><strong>Procedure</strong></p><p>Details of the error can be analyzed in details using executor logs available in stdout</p><p><img src="../../../src/site/markdown/images/query_failure_procedure.png?raw=true" alt="Query Failure Spark UI" /></p><p>Below snapshot shows executor logs with error message for query failure which can be helpful to locate the error</p><p><img src="../../../src/site/markdown/images/query_failure_issue.png?raw=true" alt="Query Failure Spark UI" /> </p><h2>Failed to execute load query on cluster.</h2><p><strong>Symptom</strong></p><p>Load query failed with the following exception:</p><p><code>
  Dictionary file is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>The carbon.properties file is not identical in all the nodes of the cluster.</p><p><strong>Procedure</strong></p><p>Follow the steps to ensure the carbon.properties file is consistent across all the nodes:</p>
<ol>
  <li><p>Copy the carbon.properties file from the master node to all the other nodes in the cluster.  For example, you can use ssh to copy this file to all the nodes.</p></li>
  <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to execute insert query on cluster.</h2><p><strong>Symptom</strong></p><p>Load query failed with the following exception:</p><p><code>
  Dictionary file is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>The carbon.properties file is not identical in all the nodes of the cluster.</p><p><strong>Procedure</strong></p><p>Follow the steps to ensure the carbon.properties file is consistent across all the nodes:</p>
<ol>
  <li><p>Copy the carbon.properties file from the master node to all the other nodes in the cluster.  For example, you can use scp to copy this file to all the nodes.</p></li>
  <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to connect to hiveuser with thrift</h2><p><strong>Symptom</strong></p><p>We get the following exception :</p><p><code>
  Cannot connect to hiveuser.
</code></p><p><strong>Possible Cause</strong></p><p>The external process does not have permission to access.</p><p><strong>Procedure</strong></p><p>Ensure that the Hiveuser in mysql must allow its access to the external processes.</p><h2>Failure to read the metastore db during table creation.</h2><p><strong>Symptom</strong></p><p>We get the following exception on trying to connect :</p><p><code>
  Cannot read the metastore db
</code></p><p><strong>Possible Cause</strong></p><p>The metastore db is dysfunctional.</p><p><strong>Procedure</strong></p><p>Remove the metastore db from the carbon.metastore in the Spark Directory.</p><h2>Failed to load data on the cluster</h2><p><strong>Symptom</strong></p><p>Data loading fails with the following exception :</p><p><code>
   Data Load failure exeception
</code></p><p><strong>Possible Cause</strong></p><p>The following issue can cause the failure :</p>
<ol>
  <li><p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent across all nodes of the cluster.</p></li>
  <li><p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p></li>
</ol><p><strong>Procedure</strong></p><p>Follow the steps to ensure the following configuration files are consistent across all the nodes:</p>
<ol>
  <li><p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master node to all the other nodes in the cluster.  For example, you can use scp to copy this file to all the nodes.</p><p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p></li>
  <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to insert data on the cluster</h2><p><strong>Symptom</strong></p><p>Insertion fails with the following exception :</p><p><code>
   Data Load failure exeception
</code></p><p><strong>Possible Cause</strong></p><p>The following issue can cause the failure :</p>
<ol>
  <li><p>The core-site.xml, hive-site.xml, yarn-site and carbon.properties are not consistent across all nodes of the cluster.</p></li>
  <li><p>Path to hdfs ddl is not configured correctly in the carbon.properties.</p></li>
</ol><p><strong>Procedure</strong></p><p>Follow the steps to ensure the following configuration files are consistent across all the nodes:</p>
<ol>
  <li><p>Copy the core-site.xml, hive-site.xml, yarn-site,carbon.properties files from the master node to all the other nodes in the cluster.  For example, you can use scp to copy this file to all the nodes.</p><p>Note : Set the path to hdfs ddl in carbon.properties in the master node.</p></li>
  <li><p>For the changes to take effect, restart the Spark cluster.</p></li>
</ol><h2>Failed to execute Concurrent Operations.</h2><p><strong>Symptom</strong></p><p>Execution of Concurrent Operations (Load,Insert,Update) on table by multiple workers fails with the following exception :</p><p><code>
   Table is locked for updation.
</code></p><p><strong>Possible Cause</strong></p><p>Concurrency not supported.</p><p><strong>Procedure</strong></p><p>Worker must wait for the query execution to complete and the table to release the lock for another query execution to succeed..</p><h2>Failed to create a table with a single numeric column.</h2><p><strong>Symptom</strong></p><p>Execution fails with the following exception :</p><p><code>
   Table creation fails.
</code></p><p><strong>Possible Cause</strong></p><p>Behavior not supported.</p><p><strong>Procedure</strong></p><p>A single column that can be considered as dimension is mandatory for table creation.</p><h2>Data Failure because of Bad Records</h2><p><strong>Symptom</strong></p><p>Data Loading fails with the following exception</p><p><code>
   Error: java.lang.Exception: Data load failed due to Bad record
</code></p><p><strong>Possible Causes</strong></p><p>The parameter BAD_RECORDS_ACTION has not been specified in the Query.</p><p><strong>Procedure</strong></p><p>Set the following parameter in the load command OPTIONS as shown below :</p><p>'BAD_RECORDS_ACTION'='FORCE?</p><p><em>Example :</em></p><p><code>
   LOAD DATA INPATH &#39;hdfs://hacluster/user/loader/moredata01.csv&#39; INTO TABLE flow_carbon_256b OPTIONS(&#39;DELIMITER&#39;=&#39;,&#39;, &#39;BAD_RECORDS_ACTION&#39;=&#39;FORCE&#39;);
</code></p>